{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10d6b62",
   "metadata": {},
   "source": [
    "# Microplastics Blind Correction Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for processing microplastics data with blank and blind correction using separate Excel files. The new modular architecture allows for flexible processing of particle data from various sources.\n",
    "\n",
    "## Overview\n",
    "- Load actual sample data from Excel files\n",
    "- Load corresponding blind sample data from separate Excel files\n",
    "- Verify data structure consistency\n",
    "- Apply processing pipeline (filtering, standardization)\n",
    "- Perform blank and blind corrections\n",
    "- Visualize results and generate reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67976ef9",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import pandas, openpyxl, and other necessary libraries for Excel file handling and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e1dbcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'microplas_blind_corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mcwd() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Import our microplastics processing modules\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmicroplas_blind_corr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ExcelLoader,\n\u001b[1;32m     15\u001b[0m     ParticleProcessor,\n\u001b[1;32m     16\u001b[0m     BlankCorrector,\n\u001b[1;32m     17\u001b[0m     BlindCorrector,\n\u001b[1;32m     18\u001b[0m     ProcessingConfig\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmicroplas_blind_corr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EXCEL_COLUMN_MAPPING\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmicroplas_blind_corr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     validate_dataframe_structure,\n\u001b[1;32m     23\u001b[0m     calculate_particle_statistics,\n\u001b[1;32m     24\u001b[0m     generate_processing_report,\n\u001b[1;32m     25\u001b[0m     FileOrganizer\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'microplas_blind_corr'"
     ]
    }
   ],
   "source": [
    "# Import required libraries for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the src directory to Python path for importing our package\n",
    "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "\n",
    "# Import our microplastics processing modules\n",
    "from microplas_blind_corr import (\n",
    "    ExcelLoader,\n",
    "    ParticleProcessor,\n",
    "    BlankCorrector,\n",
    "    BlindCorrector,\n",
    "    ProcessingConfig\n",
    ")\n",
    "from microplas_blind_corr.config import EXCEL_COLUMN_MAPPING\n",
    "from microplas_blind_corr.utils import (\n",
    "    validate_dataframe_structure,\n",
    "    calculate_particle_statistics,\n",
    "    generate_processing_report,\n",
    "    FileOrganizer\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä Using pandas version: {pd.__version__}\")\n",
    "print(f\"üìà Using matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e7667",
   "metadata": {},
   "source": [
    "## Section 2: Read Actual Sample Data\n",
    "\n",
    "Load the Excel sheet containing the actual sample data using pandas read_excel() function. In this example, we'll use the provided test data which represents an environmental sample with 18,516 particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Excel loader\n",
    "loader = ExcelLoader(EXCEL_COLUMN_MAPPING)\n",
    "\n",
    "# Load the actual sample data\n",
    "sample_file = \"data/250606_Sterni_500_5_Particle_List.xlsx\"\n",
    "sample_name = \"Environmental_Sample_001\"\n",
    "\n",
    "print(f\"üìÅ Loading actual sample data from: {sample_file}\")\n",
    "\n",
    "try:\n",
    "    # Load the sample data\n",
    "    actual_sample_data = loader.load_sample(sample_file, sample_name)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded sample data!\")\n",
    "    print(f\"üìä Sample: {sample_name}\")\n",
    "    print(f\"üî¨ Particles: {len(actual_sample_data):,}\")\n",
    "    print(f\"üìã Columns: {len(actual_sample_data.columns)}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"\\nüìà Data Overview:\")\n",
    "    print(f\"   ‚Ä¢ Unique polymers: {actual_sample_data['polymer_type'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique colors: {actual_sample_data['color'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique shapes: {actual_sample_data['shape'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Size range: {actual_sample_data['size_1_um'].min():.1f} - {actual_sample_data['size_1_um'].max():.1f} Œºm\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nüìã First 5 particles:\")\n",
    "    display_cols = ['particle_id', 'polymer_type', 'color', 'shape', 'size_1_um', 'size_2_um']\n",
    "    print(actual_sample_data[display_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading sample data: {e}\")\n",
    "    actual_sample_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba90f68",
   "metadata": {},
   "source": [
    "## Section 3: Read Blind Sample Data\n",
    "\n",
    "Load the separate Excel sheet containing the corresponding blind sample data with the same structure. In a real workflow, you would have separate Excel files for each blind sample, but for this demonstration, we'll create a simulated blind sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real scenario, you would load blind sample data from separate Excel files like this:\\n# blind_files = [\\\"data/blinds/blind_001_particles.xlsx\\\", \\\"data/blinds/blind_002_particles.xlsx\\\"]\\n# blind_data = loader.load_multiple_samples(blind_files, [\\\"Blind_Sample_001\\\", \\\"Blind_Sample_002\\\"])\\n\\n# For this demonstration, we'll create simulated blind sample data\\n# by sampling from the actual data to show the workflow\\n\\nif actual_sample_data is not None:\\n    print(\\\"üé≠ Creating simulated blind sample data for demonstration...\\\")\\n    \\n    # Create a simulated blind sample by sampling from the actual data\\n    # In reality, this would come from separate Excel files\\n    np.random.seed(42)  # For reproducible results\\n    \\n    # Sample about 1% of particles to simulate a typical blind sample size\\n    blind_sample_size = max(50, len(actual_sample_data) // 100)  # At least 50 particles\\n    blind_indices = np.random.choice(actual_sample_data.index, size=blind_sample_size, replace=False)\\n    \\n    # Create blind sample data with modified sample name\\n    blind_sample_data = actual_sample_data.loc[blind_indices].copy()\\n    blind_sample_data['sample_name'] = 'Blind_Sample_001'\\n    \\n    # Modify particle IDs to make them unique\\n    blind_sample_data['particle_id'] = 'BLIND_' + blind_sample_data['particle_id'].astype(str)\\n    \\n    print(f\\\"‚úÖ Created simulated blind sample!\\\")\\n    print(f\\\"üìä Blind sample: Blind_Sample_001\\\")\\n    print(f\\\"üî¨ Particles: {len(blind_sample_data):,}\\\")\\n    print(f\\\"üìã Structure matches actual sample: {list(blind_sample_data.columns) == list(actual_sample_data.columns)}\\\")\\n    \\n    # Display blind sample info\\n    print(\\\"\\\\nüìà Blind Sample Overview:\\\")\\n    print(f\\\"   ‚Ä¢ Unique polymers: {blind_sample_data['polymer_type'].nunique()}\\\")\\n    print(f\\\"   ‚Ä¢ Unique colors: {blind_sample_data['color'].nunique()}\\\")\\n    print(f\\\"   ‚Ä¢ Unique shapes: {blind_sample_data['shape'].nunique()}\\\")\\n    print(f\\\"   ‚Ä¢ Size range: {blind_sample_data['size_1_um'].min():.1f} - {blind_sample_data['size_1_um'].max():.1f} Œºm\\\")\\n    \\n    print(\\\"\\\\nüìã First 5 blind particles:\\\")\\n    print(blind_sample_data[display_cols].head())\\n    \\nelse:\\n    print(\\\"‚ùå Cannot create blind sample data - actual sample data not available\\\")\\n    blind_sample_data = None\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834ff87",
   "metadata": {},
   "source": [
    "## Section 4: Verify Sheet Structure Consistency\n",
    "\n",
    "Check that both Excel sheets have the same column names, data types, and overall structure. This is crucial for ensuring that the processing pipeline can handle both datasets consistently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "diss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
